{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#读取数据\n",
    "batch_size=256\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#转变为符合格式的图片\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self,x):\n",
    "        return x.view(-1,1,28,28)\n",
    "\n",
    "#Flatten就是把输入批量的那一维度不变，其它维度拉成一个向量\n",
    "net=nn.Sequential(\n",
    "    Reshape(),\n",
    "    nn.Conv2d(1,6,kernel_size=5,padding=2),\n",
    "    nn.Tanh(),\n",
    "    nn.AvgPool2d(kernel_size=2,stride=2),\n",
    "    nn.Conv2d(6,16,kernel_size=5),\n",
    "    nn.Tanh(),\n",
    "    nn.AvgPool2d(kernel_size=2,stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16*5*5,120),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120,84),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84,10)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape output shape:\t torch.Size([1, 1, 28, 28])\n",
      "Conv2d output shape:\t torch.Size([1, 6, 28, 28])\n",
      "Tanh output shape:\t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape:\t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape:\t torch.Size([1, 16, 10, 10])\n",
      "Tanh output shape:\t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape:\t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape:\t torch.Size([1, 400])\n",
      "Linear output shape:\t torch.Size([1, 120])\n",
      "Sigmoid output shape:\t torch.Size([1, 120])\n",
      "Linear output shape:\t torch.Size([1, 84])\n",
      "Sigmoid output shape:\t torch.Size([1, 84])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[ 0.2728, -0.3562,  0.0868, -0.4422,  0.1509,  0.3348,  0.7059,  0.0368,\n          0.3686, -0.1758]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看层中各输入输出维度的大小\n",
    "X=torch.rand(size=(1,1,28,28),dtype=torch.float32)\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net,data_iter,device=None):\n",
    "    if isinstance(net,torch.nn.Module):\n",
    "        net.eval()#设置为评估模式\n",
    "        if not device:\n",
    "            device=next(iter(net.parameters())).device\n",
    "    #正确预测的数量和总预测的数量\n",
    "    metric = d2l.Accumulator(2)\n",
    "    for X,y in data_iter:\n",
    "        if isinstance(X,list):\n",
    "            #BERT微调所需要的\n",
    "            X = [x.to(device) for x in X]\n",
    "        else:\n",
    "            X=X.to(device)\n",
    "        y=y.to(device)\n",
    "        metric.add(d2l.accuracy(net(X),y),y.numel())\n",
    "    return metric[0]/metric[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#针对不同的设计网络的时候优化器和loss函数是自己定义的\n",
    "def Train(net,train_iter,test_iter,num_epochs,lr,device):\n",
    "    def init_weight(m):\n",
    "        if(type(m)==nn.Linear or type(m)==nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight)#均匀分布初始化权重\n",
    "\n",
    "    net.apply(init_weight)\n",
    "    #打印正在运行的是GPU还是CPU\n",
    "    print('training on',device)\n",
    "    net.to(device)#网络已经放到GPU上了\n",
    "    #定义优化器\n",
    "    optimizer=torch.optim.SGD(net.parameters(),lr=lr)\n",
    "    #定义损失函数：分类问题使用交叉熵\n",
    "    loss=nn.CrossEntropyLoss()\n",
    "\n",
    "    #画出训练图像\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练损失之和，训练准确率之和，范例数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()#模型训练前的固定语句\n",
    "        for i,(X,y) in enumerate(train_iter):\n",
    "            timer.start()#计时器\n",
    "            optimizer.zero_grad()\n",
    "            #数据放到GPU上\n",
    "            X,y=X.to(device),y.to(device)\n",
    "            y_hat=net(X)#前向传播\n",
    "            l=loss(y_hat,y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (train_l, train_acc, None))\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.296, train acc 0.889, test acc 0.873\n",
      "17213.6 examples/sec on cuda\n"
     ]
    }
   ],
   "source": [
    "lr,num_epochs=0.9,10\n",
    "Train(net,train_iter,test_iter,num_epochs,lr,torch.device('cuda'))\n",
    "torch.save(net,'./LeNet.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#实验的一个发现:将Sigmoid改为Tanh后精度提升接近6各百分点\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}